{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5 - Natural Language Processing\n",
    "#### Exploring the NLP pipeline in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4106 Artificial Intelligence  \n",
    "Fall 2019  \n",
    "Prepared by Caroline Barri√®re and Julian Templeton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***INTRODUCTION***:  \n",
    "\n",
    "This notebook is split in two parts.  In **Part A**, you will explore the different steps of the NLP pipeline, and in **Part B**, you will revisit some of the Machine Learning algorithms used in prior notebooks to perform Polarity Detection on Rotten Tomatoe reviews with NLP techniques used on the reviews.  We will work with the package *nltk* which is very useful for NLP analysis.  You will need to install it before you start.  Information about NLTK are here: http://www.nltk.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HOMEWORK***:  \n",
    "Go through the notebook by running each cell, one at a time.  \n",
    "Look for **(TO DO)** for the tasks that you need to perform. Do not edit the code outside of the questions which you are asked to answer unless specifically asked. Once you're done, Sign the notebook (at the end of the notebook), and submit it.  \n",
    "\n",
    "*The notebook will be marked on 42.  \n",
    "Each **(TO DO)** has a number of points associated with it.*\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install nltk library: pip install nltk\n",
    "# Next import nltk into python\n",
    "import nltk \n",
    "# Open the installer to download nltk packages\n",
    "# This will take a moment to open. Just download all to get eevrything that would need.\n",
    "# Once downloaded you can comment this function to avoid calling when running\n",
    "#nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART A - NATURAL LANGUAGE PROCESSING PIPELINE**  \n",
    "  \n",
    "In this part, we will use the modules from *nltk* to perform the different steps of the pipeline.  \n",
    "We first define a small sample text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText = \"Every week we have two A.I. lectures in this course. The course number is CSI4106. \"\\\n",
    "             \"In the course, we do many Jupyter Notebooks. Each notebook covers a different topic such as a SVM \"\\\n",
    "             \"classifier, Simulated Annealing, Depth-First Search, and many more. In class we have looked \"\\\n",
    "             \"at intricate algorithms, such as the ARC-3 algorithm and neural network learning algorithms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sampleText)\n",
    "# number of tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every', 'week', 'we', 'have', 'two', 'A.I', '.', 'lectures', 'in', 'this', 'course', '.', 'The', 'course', 'number', 'is', 'CSI4106', '.', 'In', 'the', 'course', ',', 'we', 'do', 'many', 'Jupyter', 'Notebooks', '.', 'Each', 'notebook', 'covers', 'a', 'different', 'topic', 'such', 'as', 'a', 'SVM', 'classifier', ',', 'Simulated', 'Annealing', ',', 'Depth-First', 'Search', ',', 'and', 'many', 'more', '.', 'In', 'class', 'we', 'have', 'looked', 'at', 'intricate', 'algorithms', ',', 'such', 'as', 'the', 'ARC-3', 'algorithm', 'and', 'neural', 'network', 'learning', 'algorithms', '.']\n"
     ]
    }
   ],
   "source": [
    "# Showing the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a - Stemming (Porter Stemmer)\n",
    "For a reference to the [algorithm](http://snowballstem.org/algorithms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everi', 'week', 'we', 'have', 'two', 'a.i', '.', 'lectur', 'in', 'thi', 'cours', '.', 'the', 'cours', 'number', 'is', 'csi4106', '.', 'In', 'the', 'cours', ',', 'we', 'do', 'mani', 'jupyt', 'notebook', '.', 'each', 'notebook', 'cover', 'a', 'differ', 'topic', 'such', 'as', 'a', 'svm', 'classifi', ',', 'simul', 'anneal', ',', 'depth-first', 'search', ',', 'and', 'mani', 'more', '.', 'In', 'class', 'we', 'have', 'look', 'at', 'intric', 'algorithm', ',', 'such', 'as', 'the', 'arc-3', 'algorithm', 'and', 'neural', 'network', 'learn', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk contains different stemmers, and we try the Porter Stemmer here\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(t) for t in tokens]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b - Lemmatization\n",
    "The lemmatization relies on a resource called Wordnet (https://wordnet.princeton.edu/), in which lemmas are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kenny.KENSURF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the wordnet resource\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every', 'week', 'we', 'have', 'two', 'A.I', '.', 'lecture', 'in', 'this', 'course', '.', 'The', 'course', 'number', 'is', 'CSI4106', '.', 'In', 'the', 'course', ',', 'we', 'do', 'many', 'Jupyter', 'Notebooks', '.', 'Each', 'notebook', 'cover', 'a', 'different', 'topic', 'such', 'a', 'a', 'SVM', 'classifier', ',', 'Simulated', 'Annealing', ',', 'Depth-First', 'Search', ',', 'and', 'many', 'more', '.', 'In', 'class', 'we', 'have', 'looked', 'at', 'intricate', 'algorithm', ',', 'such', 'a', 'the', 'ARC-3', 'algorithm', 'and', 'neural', 'network', 'learning', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(t) for t in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q1 - 2 marks**    \n",
    "Describe in your own words the difference between lemmatization and stemming.  Use examples from above to show the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 - ANSWER   \n",
    "The difference between lemmatization and stemming is that stemming tries to obtain the root of a word, where you would add affixes such as \"ed\", \"ize\", etc. while lemmatization reduces the word to the root, while also ensuring that the word belongs to the language. The difference can be seen from the above steps. \n",
    "\n",
    "For example, the word \"lectures\" becomes \"lectur\" when stemmed, removing the affix \"es\" as opposed to becoming \"lecture\" when lemmatizing, which only removed the \"s\" to get the root word \"lecture\". \n",
    "\n",
    "Another example is the word \"Simulated\". When stemming the word, we get \"Simul\" however, when we lemmatize the word, we get \"Simulated\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Part-Of-Speech tagging  (POS tagging)\n",
    "As we've seen in class, sentence splitting can be learned through a supervised model.  POS tagging can also be learned through a supervised model.  Here, we will use a perceptron model pre-trained in NLTK.  Look here http://www.nltk.org/_modules/nltk/tag/perceptron.html to understand the model.  \n",
    "  \n",
    "The full sest of tags is available [here](https://www.clips.uantwerpen.be/pages/mbsp-tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kenny.KENSURF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Every', 'DT'), ('week', 'NN'), ('we', 'PRP'), ('have', 'VBP'), ('two', 'CD'), ('A.I', 'NNP'), ('.', '.'), ('lectures', 'VBZ'), ('in', 'IN'), ('this', 'DT'), ('course', 'NN'), ('.', '.'), ('The', 'DT'), ('course', 'NN'), ('number', 'NN'), ('is', 'VBZ'), ('CSI4106', 'NNP'), ('.', '.'), ('In', 'IN'), ('the', 'DT'), ('course', 'NN'), (',', ','), ('we', 'PRP'), ('do', 'VBP'), ('many', 'JJ'), ('Jupyter', 'NNP'), ('Notebooks', 'NNP'), ('.', '.'), ('Each', 'DT'), ('notebook', 'NN'), ('covers', 'VBZ'), ('a', 'DT'), ('different', 'JJ'), ('topic', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('a', 'DT'), ('SVM', 'NNP'), ('classifier', 'NN'), (',', ','), ('Simulated', 'NNP'), ('Annealing', 'NNP'), (',', ','), ('Depth-First', 'NNP'), ('Search', 'NNP'), (',', ','), ('and', 'CC'), ('many', 'JJ'), ('more', 'JJR'), ('.', '.'), ('In', 'IN'), ('class', 'NN'), ('we', 'PRP'), ('have', 'VBP'), ('looked', 'VBN'), ('at', 'IN'), ('intricate', 'JJ'), ('algorithms', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('the', 'DT'), ('ARC-3', 'NNP'), ('algorithm', 'NN'), ('and', 'CC'), ('neural', 'JJ'), ('network', 'NN'), ('learning', 'VBG'), ('algorithms', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# nltk contains a method to obtain the part-of-speech of each token\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "posTokens = nltk.pos_tag(tokens)\n",
    "print(posTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('week', 'NN')\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "# If we just want to see one tag in particular\n",
    "print(posTokens[1])  # It's a tuple\n",
    "print(posTokens[1][1])  # Second part of the tuple is the tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to Step 2 \n",
    "\n",
    "The lemmatizer we use is based on WordNet (a lexical resource commonly used in NLP) to provide a set of lemmas.  As many words are ambiguous and can be found in sentences as verbs or nouns (remember examples such as *Will's will will be achieved*), the lemmatizer can benefit from knowledge of POS.  Small problem... POS tags in Wordnet are not the same as in Treebank.  Wordnet defines only 4 POS: N (noun), V (verb), J (adjective) and R (adverb). The small method below is to obtain a partial equivalence between the tagsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Try to lemmatize, this time knowing the POS\n",
    "# Tagsets are often different... here we map the treebank tagset (default in pos_tag) \n",
    "# to the wordnet tagset \n",
    "# We will learn more about wordnet when we discuss resources in the Knowledge Representation module of this course\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.ADV  # just use as default, for ADV the lemmatizer doesn't change anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'n', 'r', 'v', 'r', 'n', 'r', 'v', 'r', 'r', 'n', 'r', 'r', 'n', 'n', 'v', 'n', 'r', 'r', 'r', 'n', 'r', 'r', 'v', 'a', 'n', 'n', 'r', 'r', 'n', 'v', 'r', 'a', 'n', 'a', 'r', 'r', 'n', 'n', 'r', 'n', 'n', 'r', 'n', 'n', 'r', 'r', 'a', 'a', 'r', 'r', 'n', 'r', 'v', 'v', 'r', 'a', 'n', 'r', 'a', 'r', 'r', 'n', 'n', 'r', 'a', 'n', 'v', 'n', 'r']\n"
     ]
    }
   ],
   "source": [
    "# Transform the tags into wordnet tags\n",
    "wordnet_tags = [get_wordnet_pos(p[1]) for p in posTokens]\n",
    "print(wordnet_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every', 'week', 'we', 'have', 'two', 'A.I', '.', 'lecture', 'in', 'this', 'course', '.', 'The', 'course', 'number', 'be', 'CSI4106', '.', 'In', 'the', 'course', ',', 'we', 'do', 'many', 'Jupyter', 'Notebooks', '.', 'Each', 'notebook', 'cover', 'a', 'different', 'topic', 'such', 'as', 'a', 'SVM', 'classifier', ',', 'Simulated', 'Annealing', ',', 'Depth-First', 'Search', ',', 'and', 'many', 'more', '.', 'In', 'class', 'we', 'have', 'look', 'at', 'intricate', 'algorithm', ',', 'such', 'as', 'the', 'ARC-3', 'algorithm', 'and', 'neural', 'network', 'learn', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "# Now, let's try to lemmatize again, but we tell the lemmatizer what the POS is.\n",
    "posLemmas = [wnl.lemmatize(t, w) for t, w in zip(tokens, wordnet_tags)]\n",
    "print(posLemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q2 - 1 mark**    \n",
    "Which words are lemmatized differently when provided with the additional knowledge of POS? Look at the variables *lemmas* and *posLemmas* to get this answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 - ANSWER   \n",
    "Format as: <lemmatized_word> -> <POS_lemmatized_word> only for tokens that are different between posLemmas and lemmas.  \n",
    "is -> be (This may be a typo error?)\n",
    "a -> as (shown twice)\n",
    "looked -> look\n",
    "learning -> learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every week we have two A.I.', 'lectures in this course.', 'The course number is CSI4106.', 'In the course, we do many Jupyter Notebooks.', 'Each notebook covers a different topic such as a SVM classifier, Simulated Annealing, Depth-First Search, and many more.', 'In class we have looked at intricate algorithms, such as the ARC-3 algorithm and neural network learning algorithms.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence splitting can be done before tokenizing and POS tagging if we wish\n",
    "sentences = nltk.sent_tokenize(sampleText)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q3 - 2 marks**    \n",
    "How many sentences are generated?   \n",
    "Give an explanation for the number of sentences returned from sentence splitting (more, less, the same? Why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 - ANSWER   \n",
    "How many sentences were generated?   \n",
    "Six sentences were generated.\n",
    "\n",
    "Give an explanation for the number of sentences returned from sentence splitting (more, less, the same? Why?).  \n",
    "here is an additional sentence returned from sentence splitting, which occurs after the period in \"A.I.\". The period after \"I\" was identified as the end of a sentence, causing the sentence to be split into two, resulting in an additional sentence (from 5 sentences to 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together in a complete example**  \n",
    "Having covered the NLP pipeline, we will now perform these steps on a movie review from the Rotten Tomatoes Polarity Detection dataset that we used in Notebook 3.  \n",
    "  \n",
    "Recall from Notebook 3:   \n",
    "You will need to download the movie review dataset from the following shared Google Drive (or just copy it over from your notebook 3 folder!):\n",
    "https://drive.google.com/file/d/1w1TsJB-gmIkZ28d1j7sf1sqcPmHXw352/view\n",
    "\n",
    "This is a dataset of reviews from Rotten Tomatoes along with the Freshness of the review (Fresh or Rotten). We will be using this dataset throughout the notebook so be sure to place it in the same directory as this notebook. It contains 480000 reviews with half of them being rotten and the other half being fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and load the dataset\n",
    "import pandas as pd\n",
    "# Import the dataset, need to use the ISO-8859-1 encoding due to some invalid UTF-8 characters\n",
    "df = pd.read_csv(\"rt_reviews.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I can't figure out exactly when the film 10,000 B.C. is set, but it's definitely ancient times. Like before they had cars, guns or tabloid blogs. And definitely before they had cohesive plots or dialogue that made sense.\n"
     ]
    }
   ],
   "source": [
    "# We extract a specific review to be used for the remainder of this subsection\n",
    "pipeline_review = df.iloc[11992][1]\n",
    "print(pipeline_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q4 - 5 marks**    \n",
    "Perform all of the steps that we have worked with so far from the NLP Pipeline on the review pipeline_review. You will need to do all of the following:    \n",
    "1. Tokenization\n",
    "2. Lemmatization and Stemming\n",
    "3. POS Tagging\n",
    "4. POS-based lemmatization\n",
    "5. Sentence Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('ca', 'MD'), (\"n't\", 'RB'), ('figure', 'VB'), ('out', 'RP'), ('exactly', 'RB'), ('when', 'WRB'), ('the', 'DT'), ('film', 'NN'), ('10,000', 'CD'), ('B.C', 'NNP'), ('.', '.'), ('is', 'VBZ'), ('set', 'VBN'), (',', ','), ('but', 'CC'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('definitely', 'RB'), ('ancient', 'JJ'), ('times', 'NNS'), ('.', '.'), ('Like', 'IN'), ('before', 'IN'), ('they', 'PRP'), ('had', 'VBD'), ('cars', 'NNS'), (',', ','), ('guns', 'NNS'), ('or', 'CC'), ('tabloid', 'JJ'), ('blogs', 'NNS'), ('.', '.'), ('And', 'CC'), ('definitely', 'RB'), ('before', 'IN'), ('they', 'PRP'), ('had', 'VBD'), ('cohesive', 'JJ'), ('plots', 'NNS'), ('or', 'CC'), ('dialogue', 'NN'), ('that', 'IN'), ('made', 'VBD'), ('sense', 'NN'), ('.', '.')]\n",
      "MD\n",
      "46\n",
      "V\n",
      "V\n",
      "V\n",
      "V\n",
      "V\n",
      "V\n",
      "V\n"
     ]
    }
   ],
   "source": [
    "# TODO - Q4\n",
    "\n",
    "# Tokenization\n",
    "pipelineTokens = word_tokenize(pipeline_review)\n",
    "\n",
    "# Lemmatization\n",
    "pipelineLemmas = [wnl.lemmatize(t) for t in pipelineTokens]\n",
    "\n",
    "# Stemming\n",
    "pipelineStems = [stemmer.stem(t) for t in pipelineTokens]\n",
    "\n",
    "# POS Tagging\n",
    "posPipelineTokens = nltk.pos_tag(pipelineTokens)\n",
    "\n",
    "# POS-based lemmatization (Wornet tags then ...)\n",
    "wordnet_tags_pipeline = [get_wordnet_pos(p[1]) for p in posPipelineTokens]\n",
    "posPipelineLemmas = [wnl.lemmatize(t,w) for t, w in zip(pipelineTokens, wordnet_tags_pipeline)]\n",
    "\n",
    "# Sentence Splitting\n",
    "pipelineSentences = nltk.sent_tokenize(pipeline_review)"
   ]
  },
  {
   "attachments": {
    "CSI4106_Notebook5_corenlp.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Parsing\n",
    "The parser in NLTK actually is a wrapper around a java parser (from Stanford NLP).  It's a bit complex to install, so instead, we will use the following online Stanford parse tool to test pipeline_review: http://corenlp.run/   \n",
    "\n",
    "You will need to copy the review from pipeline_review (printed above) into the *Text to annotate* box, select only *parts-of-speech* and *constituency parse* in the *Annotations* box, and click the *Submit* button to generate the output that you will be looking at.  \n",
    "\n",
    "Below is an image exhbiting what was mentioned above (if not visible for some reason, the image is included in the Notebook 5 tab on Brightspace):\n",
    "\n",
    "![CSI4106_Notebook5_corenlp.PNG](attachment:CSI4106_Notebook5_corenlp.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q5 - 2 marks**    \n",
    "Submit the input as described above into the corenlp Web-based tool and answer the following questions.\n",
    "1. Part-of-Speech: How many words are tagged as a verb (in this case any tag starting with V)?\n",
    "2. Constituency Parsing: What does this output represent? How many trees are there? What do the leaves of the trees represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5 - ANSWER   \n",
    "\n",
    "1. There are seven words tagged as a verb.\n",
    "2. The output represents the syntactic structure of the review from pipeline_review. There are a total of three separate trees, one for each sentence in the review. The leaves of the trees represent the lexical tokens of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART B - Supervised Learning using the NLP Pipeline**  \n",
    "  \n",
    "In this second part, we will be re-visiting the algorithms used in notebooks 3 and 4 to perform polarity detection on the Rotten Tomatoes dataset. However, this time we will be using various NLP techniques to modify the reviews before using them as input for the algorithms.  \n",
    " \n",
    "This section will be structured in the following way:  \n",
    "1. First we will walk through how to manipulate the data that we will be using for training and testing using various NLP techniques. Then, you will be given several NLP tasks to perform on the reviews that we will use for training and testing later.   \n",
    "    1.1. NOTE: The tasks will be clearly defined with an example, but you will need to come up with a way to perform using what you have been shown through examples in this notebook.\n",
    "2. Using the modified train and test sets from (1), you will select two of the models that we have used for Supervised Machine Learning (either Naive Bayes or SVM + either Logistic Regression or MLPClassifier) to perform Polarity Detection. You will compare the test results obtained from the models that you will invoke.\n",
    "    2.1. NOTE: You are free to re-use code from Notebooks 3 and 4 to help with this task if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our training and test datasets (same as from notebook 3)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Randomly select 10000 fresh examples from the dataframe\n",
    "dfFresh = df[df[\"Freshness\"] == \"fresh\"].sample(n=10000, random_state=5)\n",
    "# Randomly select 10000 rotten examples from the dataframe\n",
    "dfRotten = df[df[\"Freshness\"] == \"rotten\"].sample(n=10000, random_state=3)\n",
    "# Combine the results to make a small random subset of reviews to use\n",
    "dfPartial = dfFresh.append(dfRotten)\n",
    "\n",
    "# Split the data such that 90% is used for training and 10% is used for testing (separating the review\n",
    "# from the freshness scores that we will use as the labels)\n",
    "# Recall that we do not use this test set when building the model, only the training set\n",
    "# We use the parameter stratify to split the training and testing data equally to create\n",
    "# a balanced dataset\n",
    "train_reviews, test_reviews, train_tags, test_tags = train_test_split(dfPartial[\"Review\"],\n",
    "                                                                      dfPartial[\"Freshness\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=10,\n",
    "                                                                      stratify=dfPartial[\"Freshness\"])\n",
    "\n",
    "# Note that we do not convert to numpy arrays here. You can still loop through the contents as normal,\n",
    "# you will just need to use the df.iloc[index] method to get a single review at an index (you will see\n",
    "# that you do not even need to do this for this notebook. Just iterating though the dataframe is all you need!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253536     You Again poses an interesting question -- wh...\n",
      "252809     Joe Swanberg's starriest picture is a lovely ...\n",
      "386907     There is very little here to disabuse the gro...\n",
      "222829     A sensitive portrait, but often a wretched on...\n",
      "418760     Nothing really happens besides self-introspec...\n",
      "                                ...                        \n",
      "395461     A two-hour roller-coaster trip, Jurassic Worl...\n",
      "308329     Mr. No√Ø¬ø¬Ω√Ø¬ø¬Ω's juxtaposition of lofty religio...\n",
      "54446      Ira Sachs is a director of uncommon empathy a...\n",
      "40587      Tackling this life with honesty and respect d...\n",
      "43566      It is a film that keeps the facts and scarcel...\n",
      "Name: Review, Length: 18000, dtype: object \n",
      "\n",
      " You Again poses an interesting question -- what if our long-ago bullies were just as psychically scarred by the tormenting as their tormented victims were? -- but that curveball is buried under a lot of gunk.\n"
     ]
    }
   ],
   "source": [
    "# Reminder that the data is a pandas dataframe which you can manipulate easily\n",
    "# You can iterate through it normally and can replace the content at an index\n",
    "# Example of iterating through a dataframe:\n",
    "# for review in train_reviews: \n",
    "#     print(review)\n",
    "# This will loop through the reviews in train_reviews just like a list\n",
    "\n",
    "# Print train_reviews (dataframe of reviews)\n",
    "print(train_reviews, \"\\n\")\n",
    "# Print the first train review\n",
    "print(train_reviews.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Manipulating the movie reviews**   \n",
    "The first task will be to give an example on how to modify the movie reviews that we will be using to perform Polarity Detection. Our goal for this step will be to introduce the idea of *stopword removal* and how to perform stopword removal on every movie review that will be used for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.1. Performing Stopword Removal***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leading into the idea of stopword removal, let us first exhibit how to edit a review with respect to some task. \n",
    "\n",
    "To do this we will go through an example that removes all non-alphanumeric characters in a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  You Again poses an interesting question -- what if our long-ago bullies were just as psychically scarred by the tormenting as their tormented victims were? -- but that curveball is buried under a lot of gunk. \n",
      "\n",
      "Look at our tokenized review:\n",
      "['you', 'again', 'poses', 'an', 'interesting', 'question', '--', 'what', 'if', 'our', 'long-ago', 'bullies', 'were', 'just', 'as', 'psychically', 'scarred', 'by', 'the', 'tormenting', 'as', 'their', 'tormented', 'victims', 'were', '?', '--', 'but', 'that', 'curveball', 'is', 'buried', 'under', 'a', 'lot', 'of', 'gunk', '.'] \n",
      "\n",
      "Look at our alphanumeric tokenized review:\n",
      "['you', 'again', 'poses', 'an', 'interesting', 'question', 'what', 'if', 'our', 'bullies', 'were', 'just', 'as', 'psychically', 'scarred', 'by', 'the', 'tormenting', 'as', 'their', 'tormented', 'victims', 'were', 'but', 'that', 'curveball', 'is', 'buried', 'under', 'a', 'lot', 'of', 'gunk'] \n",
      "\n",
      "Look at our alphanumeric review:\n",
      "you again poses an interesting question what if our bullies were just as psychically scarred by the tormenting as their tormented victims were but that curveball is buried under a lot of gunk\n"
     ]
    }
   ],
   "source": [
    "# Import the Regular expressions library which is used for this example\n",
    "import re\n",
    "\n",
    "# We will use the first review from our training set as an example.\n",
    "ex_review = train_reviews.iloc[0]\n",
    "print(\"Review:\", ex_review, \"\\n\")\n",
    "\n",
    "# We need to tokenize the review first\n",
    "# Recall that we could do this to all reviews in the dataframe by iterating through its\n",
    "# contents the same way as we would a list (for review in dataframe)\n",
    "ex_review_tok = word_tokenize(ex_review.lower())\n",
    "print(\"Look at our tokenized review:\")\n",
    "print(ex_review_tok, \"\\n\")\n",
    "\n",
    "# We then loop through the tokens, keeping only the alphanumeric tokens\n",
    "ex_review_tok_alpha = [t for t in ex_review_tok if re.match(\"^[a-zA-Z]+$\", t)]\n",
    "print(\"Look at our alphanumeric tokenized review:\")\n",
    "print(ex_review_tok_alpha, \"\\n\")\n",
    "\n",
    "# Join the tokens to re-form the sentence\n",
    "ex_review_text = \" \".join(ex_review_tok_alpha)\n",
    "print(\"Look at our alphanumeric review:\")\n",
    "print(ex_review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q6 - 2 marks**    \n",
    "In the above example we removed any non-alphanumeric tokens from our selected review and re-constructed the review. If we want to build a modified training set based on these re-constructed reviews we would need to perform this operation on every review and either replace the original review or create a new list of the re-constructed reviews.  \n",
    "\n",
    "Now, we want to build a new version of both the train and the test reviews with any non-alphanumeric tokens and any *stopwords* removed. A stopword is a word that is too common to be deemed useful when training or testing a model. In nltk, we will use the stopwords list available as a guide on what to *remove*. \n",
    "\n",
    "Complete the code below to create a new version of train_reviews and test_reviews where all reviews do not contain any non-alphanumeric tokens or stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kenny.KENSURF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Q6 - Importing the stopwords and looking at what they are\n",
    "\n",
    "# Download the stopwords package if you do not already have it\n",
    "nltk.download('stopwords')\n",
    "# Import the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Can uncomment to view what the stopwords look like\n",
    "#setStopWords = set(stopwords.words('english'))\n",
    "#print(setStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 - TODO\n",
    "\n",
    "# From a given train or test set, returns a new version of the set (as a numpy array)\n",
    "# with any stopwords or non-alphanumeric characters removed\n",
    "def no_alpnum_no_stopword(reviews):\n",
    "    new_reviews = []\n",
    "    for review in reviews:\n",
    "        # Tokenize\n",
    "        new_review_tok = word_tokenize(review.lower())\n",
    "        # Remove non-alphanumeric characters\n",
    "        new_review_tok_alpha = [t for t in new_review_tok if re.match(\"^[a-zA-Z]+$\", t)]\n",
    "        # Remove stopwords after removing non-alphanumeric characters\n",
    "        new_review_tok_alpha_stopwords = [w for w in new_review_tok_alpha if not w in stopwords.words('english')]\n",
    "        # Re-form the tokens\n",
    "        new_review_text=\"\".join(new_review_tok_alpha)\n",
    "        # Append to new_reviews\n",
    "        new_reviews.append(new_review_text)\n",
    "    return np.array(new_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a few minutes so be patient!\n",
    "# New version of the train set (just modifying the reviews, not the tags)\n",
    "train_reviews_sw = no_alpnum_no_stopword(train_reviews)\n",
    "# New version of the test set (just modifying the reviews, not the tags)\n",
    "test_reviews_sw = no_alpnum_no_stopword(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.2. Performing POS Lemmatization***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q7 - 3 marks**    \n",
    "In the above example we removed any non-alphanumeric tokens and stopwords from train_reviews and test_reviews. You will now need to define the function pos_lemmatize_reviews() below. This function will take as input either *train_reviews* or *test_reviews* and will return the numpy array of the *POS Lemmatized* reviews.\n",
    "\n",
    "Everything needed to perform this task has already been done in this notebook, so you only need to use what we have covered in this notebook to answer the question.\n",
    "\n",
    "Complete the code below to create a new version of train_reviews and test_reviews where all reviews have had POS Lemmatization performed on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 - TODO\n",
    "\n",
    "# From a given train or test set, returns a new version of the set (as a numpy array)\n",
    "# with reviews that have been POS Lemmatized\n",
    "def pos_lemmatize_reviews(reviews):\n",
    "    new_pos_reviews = []\n",
    "    for review in reviews:\n",
    "        #Tokenization\n",
    "        plr_tokens = word_tokenize(review)\n",
    "        #POS Tagging\n",
    "        plr_pos_tokens = nltk.pos_tag(plr_tokens)\n",
    "        #POS-based Lemmatization\n",
    "        wordnet_tags_plr = [get_wordnet_pos(p[1]) for p in plr_pos_tokens]\n",
    "        plr_pos_lemmas = [wnl.lemmatize(t,w) for t, w in zip(plr_tokens, wordnet_tags_plr)]\n",
    "        # Re-form tokens\n",
    "        new_pos_text = \"\".join(plr_pos_lemmas)\n",
    "        #Append to new POS reviews\n",
    "        new_pos_reviews.append(new_pos_text)\n",
    "    #Numpy Array\n",
    "    return np.array(new_pos_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version of the train set (just modifying the reviews, not the tags)\n",
    "train_reviews_lm = pos_lemmatize_reviews(train_reviews)\n",
    "# New version of the test set (just modifying the reviews, not the tags)\n",
    "test_reviews_lm = pos_lemmatize_reviews(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.3. Verb-only reviews***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q8 - 3 marks**    \n",
    " You will now need to define the function verb_only_reviews() below. This function will take as input either train_reviews or test_reviews and will return the numpy array of the reviews with each review containing only verbs.\n",
    "\n",
    "Everything needed to perform this task has already been done in this notebook, so you only need to use what we have covered in this notebook to answer the question.\n",
    "\n",
    "Complete the code below to create a new version of train_reviews and test_reviews where all reviews only contain verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 - TODO\n",
    "\n",
    "# From a given train or test set, returns a new version of the set (as a numpy array)\n",
    "# with reviews that only contain verbs\n",
    "def verb_only_reviews(reviews):\n",
    "    new_verb_reviews = []\n",
    "    tmp_verb = []\n",
    "    for review in reviews:\n",
    "        #Tokenization\n",
    "        verb_tokens = word_tokenize(review)\n",
    "        #POS Tagging\n",
    "        verb_pos_tokens = nltk.pos_tag(verb_tokens)\n",
    "        \n",
    "        for i in range(len(verb_pos_tokens)):\n",
    "            if(verb_pos_tokens[i][1][0].lower() == \"v\"):\n",
    "                #reforms tokens with POS starting with V (verb).\n",
    "                new_verb_text = \"\".join(verb_pos_tokens[i][0])\n",
    "        #Append to new POS review\n",
    "        new_verb_reviews.append(new_verb_text)\n",
    "        \n",
    "    #Numpy Array\n",
    "    return np.array(new_verb_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New version of the train set (just modifying the reviews, not the tags)\n",
    "train_reviews_vb = verb_only_reviews(train_reviews)\n",
    "# New version of the test set (just modifying the reviews, not the tags)\n",
    "test_reviews_vb = verb_only_reviews(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Polarity Detection using augmented train and test sets**   \n",
    "Now that we have three augmented versions of our original train and test sets we will test them out to see how they compare to one another. You will be provided two algorithm choices in each question where you will need to select one of the algorithms and perform polarity detection on the reviews using the chosen algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2.1. Naive Bayes or SVM***    \n",
    "For this subsection you will perform polarity detection on the augmented movie reviews with *either* the Naive Bayes Classifier or the SVM Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q9 - 9 marks**    \n",
    "For this question you will be selecting *either* the Naive Bayes Classifier (MultinomialNB) used in Notebook 3 or the SVM Classifier used in Notebook 3. You may need to refer to Notebook 3 to help answer this question. Your first task is to select the classifier that you would like to use.   \n",
    "\n",
    "*Note*: You are expected to use only one. If you use only more than one only the first will be marked.\n",
    "\n",
    "For each augmented set of the train and test reviews perform the following:\n",
    "\n",
    "0. Recall that the augmented train and test sets are as followed (you need to perform all of the following with each augmented set):   \n",
    "    0.1. (train_reviews_sw, train_tags), (test_reviews_sw, test_tags)  \n",
    "    0.2. (train_reviews_lm, train_tags), (test_reviews_lm, test_tags)  \n",
    "    0.3. (train_reviews_vb, train_tags), (test_reviews_vb, test_tags)  \n",
    "1. Need to use CountVectorizer() to transform each review set (requires a CountVectorizer() for each set to be fit_tansform() with the train set). The train reviews require .fit_transform() to be used with the CountVectorizer, the test reviews require .transform() to be used with the CountVectorizer.\n",
    "2. Using the parameter setup from Notebook 3, setup and train three different classifiers of your selected classifier type (one for each augmented train/test set). Recall that we train by calling model.fit(train_counts, train_tags) ***Be sure to keep each trained model as a unique variable since you will use them later***\n",
    "3. Using the test sets and the models, test your models by predicting the sentiments from the test set and print the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Q9 - 1\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Recall that we fit on the training set: ... = count_vect.fit_transform(train_reviews)\n",
    "# Recall that we transform the testing set: ... = count_vect.transform(test_reviews)\n",
    "count_vect_sw = CountVectorizer()\n",
    "train_counts_sw = count_vect_sw.fit_transform(train_reviews_sw)\n",
    "test_counts_sw = count_vect_sw.transform(test_reviews_sw)\n",
    "# Use on both the train and the test sets\n",
    "count_vect_lm = CountVectorizer()\n",
    "train_counts_lm = count_vect_lm.fit_transform(train_reviews_lm)\n",
    "test_counts_lm = count_vect_lm.transform(test_reviews_lm)\n",
    "# Use on both the train and the test sets\n",
    "count_vect_vb = CountVectorizer()\n",
    "train_counts_vb = count_vect_vb.fit_transform(train_reviews_vb)\n",
    "test_counts_vb = count_vect_vb.transform(test_reviews_vb)\n",
    "# Use on both the train and the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Q9 - 2\n",
    "from sklearn.naive_bayes import MultinomialNB # For Naive Bayes\n",
    "# You will use this model for Naive Bayes Classification\n",
    "# clf_nb = MultinomialNB()\n",
    "\n",
    "from sklearn import svm # For the SVM\n",
    "# You will use this model for SVM Classification\n",
    "# Do not forget to set the random_state to 0\n",
    "# clf_svm = svm.SVC(kernel=\"linear\", random_state=0, max_iter=3500)\n",
    "\n",
    "# Create models of your selected classifier type and train them on each train set.\n",
    "# Recall that we trin by calling model.fit(train_counts, train_tags) (variable names may differ)\n",
    "# ***Do not forget to use the provided structure above for your selected model.\n",
    "\n",
    "clf_nb_sw = MultinomialNB().fit(train_counts_sw, train_tags)\n",
    "clf_nb_lm = MultinomialNB().fit(train_counts_lm, train_tags)\n",
    "clf_nb_vb = MultinomialNB().fit(train_counts_vb, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-alphanumeric token removal and stopword removal result:\n",
      "52.1% accuracy.\n",
      "\n",
      "POS Lemmatization result:\n",
      "56.89999999999999% accuracy.\n",
      "\n",
      "Verbs only result:\n",
      "55.50000000000001% accuracy.\n"
     ]
    }
   ],
   "source": [
    "# TO DO Q9 - 3\n",
    "\n",
    "# Test each trained model using the respective test set\n",
    "# Recall that we call ... = model.predict(test_counts) to get the predictions for our test set\n",
    "prediction_sw = clf_nb_sw.predict(test_counts_sw)\n",
    "prediction_lm = clf_nb_lm.predict(test_counts_lm)\n",
    "prediction_vb = clf_nb_vb.predict(test_counts_vb)\n",
    "\n",
    "# Print the test accuracies of each model using the predictions that you have obtained above\n",
    "print(\"Non-alphanumeric token removal and stopword removal result:\")\n",
    "correct_sw = 0\n",
    "for tag, pred in zip(test_tags, prediction_sw):   # zip allows to go through two lists simultaneously\n",
    "    if (tag == pred):\n",
    "        correct_sw += 1\n",
    "print(str(correct_sw/test_tags.size*100)+\"% accuracy.\\n\")\n",
    "\n",
    "correct_lm = 0\n",
    "for tag, pred in zip(test_tags, prediction_lm):   # zip allows to go through two lists simultaneously\n",
    "    if (tag == pred):\n",
    "        correct_lm += 1\n",
    "print(\"POS Lemmatization result:\")\n",
    "print(str(correct_lm/test_tags.size*100)+\"% accuracy.\\n\")\n",
    "\n",
    "correct_vb = 0\n",
    "for tag, pred in zip(test_tags, prediction_vb):   # zip allows to go through two lists simultaneously\n",
    "    if (tag == pred):\n",
    "        correct_vb += 1\n",
    "print(\"Verbs only result:\")\n",
    "print(str(correct_vb/test_tags.size*100)+\"% accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q10 - 2 marks**    \n",
    "Which NLP technique performed on the reviews gave the best test results above? Why do you think that approach gave the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10 - ANSWER   \n",
    "POS Lemmatization is the NLP technique that performed the best (highest accuracy). This is because the tokens were modified using the lookups in Wordnet. Alongside this, the lemmatization is more accurate when retrieving the root of words as it \"knows\" the type of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2.2. Logistic Regression or MLP***    \n",
    "For this subsection you will perform polarity detection on the augmented movie reviews with *either* the Logistic Regression or a MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q11 - 6 marks**    \n",
    "For this question you will be selecting *either* LogisticRegression as used in Notebook 4 or the MLPClassifier used in Notebook 4. You may need to refer to Notebook 4 to help answer this question. Your first task is to select the classifier that you would like to use (this time the MLP will be faster so do not worry about the computational complexity when choosing). The MLPClassifier will now use smaller hidden layer values, so use the provided structure for all of your models.\n",
    "\n",
    "*Note*: You are expected to use only one. If you use only more than one only the first will be marked.\n",
    "\n",
    "For each augmented set of the train and test reviews perform the following:\n",
    "\n",
    "0. You have already setup the transformed train and test data with the CountVectorizers from Q9. You will be re-using their values to train the model with here (so already complete)\n",
    "1. Setup and train three different classifiers of your selected classifier type (one for each augmented train/test set, for only *one epoch*). Recall that we train these models by calling model.fit(train_counts, train_tags). ***Be sure to keep each trained model as a unique variable since you will use them later***\n",
    "2. Using the test sets and the models, test your models by retrieving the score of the model predicting the test sets after being trained. Print out these scores afterwards. Recall that we get the score by calling ... = model.score(test_counts, test_tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(50, 25), learning_rate='constant',\n",
       "              learning_rate_init=0.01, max_iter=100, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO Q11 - 1\n",
    "# Recall that we train for one epoch by calling model.fit(train_counts, train_tags)\n",
    "\n",
    "# For Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# What we will use for LogisticRegression\n",
    "# clf_lr = LogisticRegression(solver='lbfgs', multi_class=\"multinomial\", max_iter=1000, random_state=1)\n",
    "\n",
    "# For MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# What we will use for the MLPClassifier\n",
    "#clf_mlp = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(50, 25), random_state=1, max_iter=100, learning_rate_init=0.01, warm_start=True)\n",
    "\n",
    "# Create models of your selected classifier type and train them on each train set.\n",
    "# We train the models the same way that we have in notebook 4, except that we only use one epoch\n",
    "# Do not forget to use the provided structure above for your selected model.\n",
    "clf_mlp_sw = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(50, 25), random_state=1, max_iter=100, learning_rate_init=0.01, warm_start=True)\n",
    "clf_mlp_lm = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(50, 25), random_state=1, max_iter=100, learning_rate_init=0.01, warm_start=True)\n",
    "clf_mlp_vb = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(50, 25), random_state=1, max_iter=100, learning_rate_init=0.01, warm_start=True)\n",
    "\n",
    "clf_mlp_sw.fit(train_counts_sw, train_tags)\n",
    "clf_mlp_lm.fit(train_counts_lm, train_tags)\n",
    "clf_mlp_vb.fit(train_counts_vb, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-alphanumeric token removal and stopword removal score\n",
      "0.501\n",
      "POS Lemmatization score\n",
      "0.5385\n",
      "Verbs only score\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "# TO DO Q11 - 2\n",
    "# Recall that we test the model by getting the score: ... = model.score(test_counts, test_tags)\n",
    "\n",
    "# Test each trained model using the respective test set\n",
    "scoreTest_sw = clf_mlp_sw.score(test_counts_sw, test_tags)\n",
    "scoreTest_lm = clf_mlp_lm.score(test_counts_lm, test_tags)\n",
    "scoreTest_vb = clf_mlp_vb.score(test_counts_vb, test_tags)\n",
    "\n",
    "# Print the test accuracies of each model using the predictions that you have obtained above\n",
    "print(\"Non-alphanumeric token removal and stopword removal score\")\n",
    "print(scoreTest_sw)\n",
    "print(\"POS Lemmatization score\")\n",
    "print(scoreTest_lm)\n",
    "print(\"Verbs only score\")\n",
    "print(scoreTest_vb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q12 - 2 marks**    \n",
    "Which NLP technique performed on the reviews gave the best test results above? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12 - ANSWER   \n",
    "Verbs only. This is because in verb only, the reviews are stripped down to just verbs, where the comparison becomes simplified as only one POS is compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q13 - 3 marks**    \n",
    "First, fill out the provided table below by entering the algorithm types that you have selected and by entering the test performance results that you have received on each modified test set. You just need to manually add them.   \n",
    "\n",
    "Did the top performing first model choice (Naive Bayes or SVM) and the top performing second model choice (Logistic Regression or MLP) both use the same NLP technique on the reviews?  \n",
    "\n",
    "What are two other NLP transformations that you can think of that would be worth testing to try and increase performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13 - ANSWER   \n",
    "\n",
    "Fill out the table below (change *result* to the test results and *Algo 1*/*Algo 2* to the selected algorithms (SVM, ...))  \n",
    "\n",
    "| Choice          | test_reviews_sw | test_reviews_lm | test_reviews_vb |\n",
    "|-----------------|-----------------|-----------------|-----------------|\n",
    "| Naive Bayes     | 0.521           | 0.569           | 0.555           |\n",
    "| MLP Classifier  | 0.501           | 0.5385          | 0.56            |     \n",
    "\n",
    "Did the top performing first model choice (Naive Bayes or SVM) and the top performing second model choice (Logistic Regression or MLP) both use the same NLP technique on the reviews?  \n",
    "\n",
    "No, the top performing model for Naive Bayes used POS Lemmatization while the top performing model for MLP Classifier used verbs only.   \n",
    "\n",
    "What are two other NLP transformations that you can think of that would be worth testing to try and increase performance?   \n",
    "\n",
    "Two other NLP transformations that may be worth testing to try and increase performance are entity disabiguation and coreference resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature\n",
    "\n",
    "I, Kenny Nguyen, declare that the answers provided in this notebook are my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
